import requests
from bs4 import BeautifulSoup
import re
from deep_translator import GoogleTranslator
import os
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet

# ä¸‹è¼‰é‚„åŸå–®å­—æ‰€éœ€çš„æ•¸æ“šåŒ…
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')
nltk.download('omw-1.4')

# å¾ GitHub Secrets è®€å–è³‡è¨Š
BOT_TOKEN = os.environ.get('BOT_TOKEN')
CHAT_ID = os.environ.get('CHAT_ID')

def get_wordnet_pos(word):
    """å°‡ nltk çš„è©æ€§æ¨™ç±¤è½‰ç‚º wordnet å¯ç”¨çš„æ¨™ç±¤"""
    tag = nltk.pos_tag([word])[0][1][0].upper()
    tag_dict = {"J": wordnet.ADJ, "N": wordnet.NOUN, "V": wordnet.VERB, "R": wordnet.ADV}
    return tag_dict.get(tag, wordnet.NOUN)

def lemmatize_word(word):
    """è©å½¢é‚„åŸï¼šå‹•è©è®ŠåŸå‹ã€åè©è®Šå–®æ•¸ï¼Œä½†ä¿ç•™ -ed å½¢å®¹è©"""
    lemmatizer = WordNetLemmatizer()
    
    # å–å¾—è©æ€§
    tag = nltk.pos_tag([word])[0][1]
    
    # å¦‚æœå·²ç¶“æ˜¯å½¢å®¹è© (JJ)ï¼Œå‰‡ç›´æ¥å›å‚³ä¸è™•ç† (ç¬¦åˆä½ æåˆ°çš„ ed æ˜¯å½¢å®¹è©æ²’é—œä¿‚)
    if tag.startswith('JJ'):
        return word
    
    # å¦å‰‡æ ¹æ“šè©æ€§é‚„åŸ
    pos = get_wordnet_pos(word)
    return lemmatizer.lemmatize(word, pos)

def get_phonetic(word):
    """ç²å–éŸ³æ¨™ (IPA)"""
    try:
        url = f"https://api.dictionaryapi.dev/api/v2/entries/en/{word}"
        response = requests.get(url, timeout=5)
        if response.status_code == 200:
            data = response.json()
            phonetic = data[0].get('phonetic')
            if not phonetic:
                phonetics = data[0].get('phonetics', [])
                for p in phonetics:
                    if p.get('text'): return p.get('text')
            return phonetic
        return ""
    except:
        return ""

def get_cnn_data(limit=10):
    url = "https://edition.cnn.com/"
    headers = {'User-Agent': 'Mozilla/5.0'}
    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.text, 'html.parser')
    headlines = [h.get_text().strip() for h in soup.find_all(['span', 'h3'], class_='container__headline-text')]
    
    results = []
    used_words = set()
    translator = GoogleTranslator(source='en', target='zh-TW')

    for sentence in headlines:
        raw_words = re.findall(r'\b[a-z]{9,}\b', sentence.lower())
        for raw_word in raw_words:
            # åŸ·è¡Œè©å½¢é‚„åŸ (åè©å»s, å‹•è©å›åŸå‹)
            word = lemmatize_word(raw_word)
            
            if word not in used_words and len(results) < limit:
                try:
                    word_cn = translator.translate(word)
                    phonetic = get_phonetic(word)
                    context_cn = translator.translate(sentence)
                    
                    results.append({
                        'word': word.capitalize(),
                        'raw_word': raw_word, # ä¿ç•™åŸå§‹å‡ºç¾çš„æ¨£å­
                        'phonetic': phonetic,
                        'translation': word_cn,
                        'context_en': sentence,
                        'context_cn': context_cn
                    })
                    used_words.add(word)
                except:
                    continue
        if len(results) >= limit: break
    return results

def send_to_telegram(items):
    if not items: return
    message = "<b>ä»Šæ—¥ CNN æ™‚äº‹å–®å­—æ¨æ’­</b> ğŸ“š\n--------------------------------\n\n"
    for i, item in enumerate(items, 1):
        phonetic_display = f" <code>{item['phonetic']}</code>" if item['phonetic'] else ""
        message += f"{i}. <b>{item['word']}</b>{phonetic_display}\n"
        message += f"   ğŸ”¹ ä¸­æ–‡ï¼š{item['translation']}\n"
        message += f"   ğŸ“ åŸå¥ï¼š<i>{item['context_en']}</i>\n"
        message += f"   ğŸ’¡ ç¿»è­¯ï¼š{item['context_cn']}\n\n"

    api_url = f"https://api.telegram.org/bot{BOT_TOKEN}/sendMessage"
    requests.post(api_url, data={"chat_id": CHAT_ID, "text": message, "parse_mode": "HTML"})

if __name__ == "__main__":
    data = get_cnn_data(10)
    send_to_telegram(data)
