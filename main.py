import requests
from bs4 import BeautifulSoup
import re
from deep_translator import GoogleTranslator
import os
import nltk
import time
import random
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet
from nltk import ne_chunk, pos_tag, word_tokenize

# --- ç’°å¢ƒåˆå§‹åŒ– ---
for pkg in ['wordnet', 'averaged_perceptron_tagger', 'averaged_perceptron_tagger_eng', 
            'omw-1.4', 'punkt', 'punkt_tab', 'maxent_ne_chunker', 'maxent_ne_chunker_tab', 'words']:
    nltk.download(pkg, quiet=True)

BOT_TOKEN = os.environ.get('BOT_TOKEN')
CHAT_ID = os.environ.get('CHAT_ID')

# æ‰‹å‹•æ””æˆªæ¸…å–®ï¼šåŒ…å«æ–°èåŸºæœ¬è·æ¥­ã€è¡Œç‚ºã€ä»¥åŠå¸¸è¦‹åœ°åäººå
MANUAL_BLOCK = {
    'lawmaker', 'lawmakers', 'voter', 'voters', 'protester', 'protesters', 'gather', 'gathers',
    'protest', 'protests', 'strike', 'strikes', 'attack', 'attacks', 'blast', 'blasts',
    'warns', 'insists', 'insist', 'claim', 'claims', 'actually', 'really', 'behind',
    'police', 'official', 'officials', 'government', 'president', 'minister', 'mayor',
    'palace', 'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday',
    'celebrity', 'famous', 'everything', 'something', 'another', 'himself', 'herself',
    'comeback', 'outside', 'inside', 'through', 'across', 'against', 'without'
}

def get_common_words(limit=5000): # é›£åº¦æå‡è‡³ 5000 å­—
    try:
        url = "https://raw.githubusercontent.com/first20hours/google-10000-english/master/google-10000-english-no-swears.txt"
        res = requests.get(url, timeout=10)
        return set(res.text.lower().splitlines()[:limit])
    except: return set()

COMMON_SET = get_common_words(5000)

def lemmatize_word(word):
    try:
        lemmatizer = WordNetLemmatizer()
        tag = pos_tag([word])[0][1]
        tag_dict = {"J": wordnet.ADJ, "N": wordnet.NOUN, "V": wordnet.VERB, "R": wordnet.ADV}
        return lemmatizer.lemmatize(word, tag_dict.get(tag[0].upper(), wordnet.NOUN))
    except: return word

def get_phonetic(word):
    try:
        url = f"https://api.dictionaryapi.dev/api/v2/entries/en/{word}"
        res = requests.get(url, timeout=5)
        if res.status_code == 200:
            return res.json()[0].get('phonetic', "")
    except: pass
    return ""

def get_news_data():
    url = "https://www.bbc.com/news"
    headers = {'User-Agent': 'Mozilla/5.0'}
    
    try:
        response = requests.get(url, headers=headers, timeout=15)
        soup = BeautifulSoup(response.text, 'html.parser')
        headlines = list(set([h.get_text().strip() for h in soup.find_all(['h2', 'h3']) if len(h.get_text().strip()) > 15]))
        print(f"--- æ­¥é©Ÿ 1: æŠ“å–åˆ° {len(headlines)} å‰‡æ¨™é¡Œ ---")

        word_pool = {}
        for sentence in headlines:
            tokens = word_tokenize(sentence)
            tagged = pos_tag(tokens)
            
            for i, (word, tag) in enumerate(tagged):
                word_lower = word.lower()
                
                # 1. åŸºç¤é•·åº¦é–€æª» (4å­—ä»¥ä¸Š)
                if len(word_lower) < 4: continue
                
                # 2. å¯¦é«”æ’é™¤é‚è¼¯ (å¤§å¯«é€šå¸¸æ˜¯åœ°åäººå)
                # å¦‚æœå–®å­—é–‹é ­å¤§å¯«ï¼Œä¸”ä¸åœ¨æˆ‘å€‘å¸¸ç”¨å­—çš„å‰ 1000 å(é¿å…æ¨™é¡Œç¬¬ä¸€å€‹å­—è¢«èª¤æ®º)ï¼Œå°±æ’é™¤
                if word[0].isupper() and word_lower not in list(COMMON_SET)[:1000]:
                    continue
                
                # 3. è©æ€§æ’é™¤ (ä»£åè©ã€æ•¸è©)
                if tag.startswith('PRP') or tag == 'CD': continue
                
                # 4. æ‰‹å‹•é»‘åå–® & 5000å­—å¸¸ç”¨å­—æ’é™¤
                if word_lower in MANUAL_BLOCK or word_lower in COMMON_SET:
                    continue
                
                # 5. è©å½¢é‚„åŸå¾Œå†æ¬¡éæ¿¾
                base = lemmatize_word(word_lower)
                if base in COMMON_SET or base in MANUAL_BLOCK or len(base) < 4:
                    continue
                
                if base not in word_pool:
                    word_pool[base] = sentence

        candidate_keys = list(word_pool.keys())
        print(f"ç¯©é¸å®Œæˆï¼šç¬¦åˆ 5000 å­—æ¨™æº–çš„å–®å­—æ•¸ç‚º {len(candidate_keys)}")
        print(f"é›£è©å€™é¸æ± é è¦½: {candidate_keys[:10]}")

        # å¦‚æœ 5000 å­—å¤ªåš´æ ¼å°è‡´å–®å­—ä¸å¤  10 å€‹ï¼Œé€€è€Œæ±‚å…¶æ¬¡ç”¨ 3000 å­—ä¿åº•
        if len(candidate_keys) < 10:
            print("é›£è©ä¸è¶³ï¼Œå•Ÿå‹•ä¿åº•è£œå……...")
            backup_set = set(list(COMMON_SET)[:3000])
            # ... (ä¿åº•é‚è¼¯)

        selected_keys = random.sample(candidate_keys, min(len(candidate_keys), 10))
        results = []
        translator = GoogleTranslator(source='en', target='zh-TW')
        
        for word in selected_keys:
            try:
                print(f"æ­£åœ¨è™•ç†: {word}")
                results.append({
                    'word': word.capitalize(),
                    'phonetic': get_phonetic(word),
                    'translation': translator.translate(word),
                    'context_en': word_pool[word],
                    'context_cn': translator.translate(word_pool[word])
                })
                time.sleep(0.3)
            except: continue
        return results
    except Exception as e:
        print(f"Error: {e}"); return []

def send_to_telegram(items):
    if not items: return
    message = "<b>ä»Šæ—¥æ™‚äº‹ç²¾é¸ï¼šæ·±åº¦é›£è© (5000å­—ç‰ˆ)</b> ğŸ“\n" + "-"*20 + "\n\n"
    for i, item in enumerate(items, 1):
        p = f" <code>{item['phonetic']}</code>" if item['phonetic'] else ""
        message += f"{i}. <b>{item['word']}</b>{p}\n   ğŸ”¹ {item['translation']}\n   ğŸ“ <i>{item['context_en']}</i>\n   ğŸ’¡ {item['context_cn']}\n\n"

    requests.post(f"https://api.telegram.org/bot{BOT_TOKEN}/sendMessage", 
                  data={"chat_id": CHAT_ID, "text": message, "parse_mode": "HTML"})

if __name__ == "__main__":
    data = get_news_data()
    send_to_telegram(data)
