import requests
from bs4 import BeautifulSoup
import re
from deep_translator import GoogleTranslator
import os

# å¾ GitHub Secrets è®€å–è³‡è¨Š
BOT_TOKEN = os.environ.get('BOT_TOKEN')
CHAT_ID = os.environ.get('CHAT_ID')

def get_kk(word):
    """å¾ Yahoo å­—å…¸æŠ“å– KK éŸ³æ¨™ (å¼·åŒ–ç‰ˆ)"""
    try:
        url = f"https://tw.dictionary.search.yahoo.com/search?p={word}"
        # æ¨¡æ“¬æ›´çœŸå¯¦çš„ç€è¦½å™¨è¡Œç‚º
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }
        res = requests.get(url, headers=headers, timeout=5)
        soup = BeautifulSoup(res.text, 'html.parser')
        
        # å°‹æ‰¾åŒ…å« KK å­—æ¨£çš„å€å¡Š
        comp_list = soup.find_all('span', class_='compList')
        for item in comp_list:
            text = item.get_text()
            if 'KK' in text:
                # åªç•™ä¸‹éŸ³æ¨™éƒ¨åˆ†ï¼Œä¾‹å¦‚ [Ã¦...]
                return text.replace('KK', '').strip()
        return ""
    except Exception as e:
        print(f"éŸ³æ¨™æŠ“å–éŒ¯èª¤ ({word}): {e}")
        return ""

def get_cnn_data(limit=10):
    url = "https://edition.cnn.com/"
    headers = {'User-Agent': 'Mozilla/5.0'}
    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.text, 'html.parser')
    # æŠ“å– CNN æ¨™é¡Œ
    headlines = [h.get_text().strip() for h in soup.find_all(['span', 'h3'], class_='container__headline-text')]
    
    results = []
    used_words = set()
    translator = GoogleTranslator(source='en', target='zh-TW')

    for sentence in headlines:
        # æŠ“å– 9 å€‹å­—æ¯ä»¥ä¸Šçš„å–®å­—
        words_in_sentence = re.findall(r'\b[a-z]{9,}\b', sentence.lower())
        for word in words_in_sentence:
            if word not in used_words and len(results) < limit:
                try:
                    word_cn = translator.translate(word)
                    kk = get_kk(word)
                    context_cn = translator.translate(sentence)
                    
                    results.append({
                        'word': word.capitalize(),
                        'kk': kk,
                        'translation': word_cn,
                        'context_en': sentence,
                        'context_cn': context_cn
                    })
                    used_words.add(word)
                    print(f"æˆåŠŸè™•ç†: {word}") # é€™æ˜¯ç‚ºäº†è®“ä½ åœ¨ GitHub Action Log è£¡çœ‹å¾—åˆ°é€²åº¦
                except:
                    continue
                    
        if len(results) >= limit: break
    return results

def send_to_telegram(items):
    if not items: 
        print("æ²’æœ‰æŠ“å–åˆ°è³‡æ–™")
        return
        
    message = "<b>ä»Šæ—¥ CNN æ™‚äº‹å–®å­—æ¨æ’­</b> ğŸ“š\n"
    message += "--------------------------------\n\n"
    
    for i, item in enumerate(items, 1):
        # é€™è£¡ä¿®æ­£äº†è®€å–éŸ³æ¨™çš„å¯«æ³•
        kk_display = f" {item['kk']}" if item['kk'] else ""
        message += f"{i}. <b>{item['word']}</b>{kk_display}\n"
        message += f"   ğŸ”¹ ä¸­æ–‡ï¼š{item['translation']}\n"
        message += f"   ğŸ“ åŸå¥ï¼š<i>{item['context_en']}</i>\n"
        message += f"   ğŸ’¡ ç¿»è­¯ï¼š{item['context_cn']}\n\n"

    api_url = f"https://api.telegram.org/bot{BOT_TOKEN}/sendMessage"
    payload = {"chat_id": CHAT_ID, "text": message, "parse_mode": "HTML"}
    
    res = requests.post(api_url, data=payload)
    if res.status_code != 200:
        print(f"TG ç™¼é€å¤±æ•—: {res.text}")

if __name__ == "__main__":
    data = get_cnn_data(10)
    send_to_telegram(data)
