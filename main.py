import requests
from bs4 import BeautifulSoup
import re
from deep_translator import GoogleTranslator
import os
import nltk
import time
import random
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet
from nltk import ne_chunk, pos_tag, word_tokenize

# --- ç’°å¢ƒåˆå§‹åŒ– ---
print("æ­£åœ¨åˆå§‹åŒ–ç’°å¢ƒèˆ‡ä¸‹è¼‰ NLTK è³‡æº...")
for pkg in ['wordnet', 'averaged_perceptron_tagger', 'averaged_perceptron_tagger_eng', 
            'omw-1.4', 'punkt', 'punkt_tab', 'maxent_ne_chunker', 'maxent_ne_chunker_tab', 'words']:
    nltk.download(pkg, quiet=True)

BOT_TOKEN = os.environ.get('BOT_TOKEN')
CHAT_ID = os.environ.get('CHAT_ID')

def get_common_words(limit=2000):
    print(f"æ­£åœ¨è¼‰å…¥å‰ {limit} å€‹å¸¸ç”¨å­—æ’é™¤è¡¨...")
    try:
        url = "https://raw.githubusercontent.com/first20hours/google-10000-english/master/google-10000-english-no-swears.txt"
        res = requests.get(url, timeout=10)
        return set(res.text.lower().splitlines()[:limit])
    except Exception as e:
        print(f"å¸¸ç”¨å­—è¼‰å…¥å¤±æ•—: {e}")
        return set()

# æ”¹å› 1000 ç¢ºä¿æˆåŠŸç‡
COMMON_FILTER = get_common_words(2000)

def lemmatize_word(word):
    try:
        lemmatizer = WordNetLemmatizer()
        tag = nltk.pos_tag([word])[0][1]
        tag_dict = {"J": wordnet.ADJ, "N": wordnet.NOUN, "V": wordnet.VERB, "R": wordnet.ADV}
        return lemmatizer.lemmatize(word, tag_dict.get(tag[0].upper(), wordnet.NOUN))
    except: return word

def get_phonetic(word):
    try:
        url = f"https://api.dictionaryapi.dev/api/v2/entries/en/{word}"
        res = requests.get(url, timeout=5)
        if res.status_code == 200:
            data = res.json()
            return data[0].get('phonetic') or (data[0].get('phonetics', [{}])[0].get('text', ""))
    except: pass
    return ""

def get_news_data():
    url = "https://www.bbc.com/news"
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
    
    print(f"--- æ­¥é©Ÿ 1: æŠ“å–ç¶²é  {url} ---")
    try:
        response = requests.get(url, headers=headers, timeout=15)
        soup = BeautifulSoup(response.text, 'html.parser')
        headlines = list(set([h.get_text().strip() for h in soup.find_all(['h2', 'h3']) if len(h.get_text().strip()) > 15]))
        print(f"æˆåŠŸæŠ“å–åˆ° {len(headlines)} å‰‡æ¨™é¡Œã€‚")
        
        word_pool = {}
        for sentence in headlines:
            # NER äººåæ’é™¤
            tokens = word_tokenize(sentence)
            tags = pos_tag(tokens)
            chunks = ne_chunk(tags)
            person_names = set()
            for chunk in chunks:
                if hasattr(chunk, 'label') and chunk.label() == 'PERSON':
                    for leaf in chunk: person_names.add(leaf[0].lower())

            # æŠ“å– 6 å€‹å­—æ¯ä»¥ä¸Š (æ”¾å¯¬ä¸€é»)
            raw_words = re.findall(r'\b[a-z]{6,}\b', sentence.lower())
            for rw in raw_words:
                if rw not in person_names and rw not in COMMON_FILTER:
                    base = lemmatize_word(rw)
                    if base not in COMMON_FILTER:
                        word_pool[base] = sentence
        
        print(f"åˆæ­¥ç¯©é¸å¾Œå‰©é¤˜é›£è©æ•¸: {len(word_pool)}")
        
        # å¼·åŠ›ä¿åº•ï¼šå¦‚æœä¸å¤  10 å€‹ï¼Œå°±ç›´æ¥æŠ“æ¨™é¡Œè£¡çš„é•·å–®å­—ï¼ˆåªé¿é–‹äººåï¼‰
        if len(word_pool) < 10:
            print("å–®å­—ä¸è¶³ï¼Œæ­£åœ¨åŸ·è¡Œä¿åº•æŠ“å–...")
            for sentence in headlines:
                for rw in re.findall(r'\b[a-z]{7,}\b', sentence.lower()):
                    base = lemmatize_word(rw)
                    if base not in person_names and base not in word_pool:
                        word_pool[base] = sentence
                    if len(word_pool) >= 20: break
        
        candidate_keys = list(word_pool.keys())
        if not candidate_keys:
            print("è‡´å‘½éŒ¯èª¤: å³ä½¿ä¿åº•ä¹ŸæŠ“ä¸åˆ°ä»»ä½•å–®å­—ã€‚")
            return []

        selected_keys = random.sample(candidate_keys, min(len(candidate_keys), 10))
        results = []
        translator = GoogleTranslator(source='en', target='zh-TW')
        
        print(f"--- æ­¥é©Ÿ 2: é–‹å§‹ç¿»è­¯èˆ‡æŸ¥è©¢éŸ³æ¨™ (å…± {len(selected_keys)} å€‹) ---")
        for word in selected_keys:
            try:
                print(f"è™•ç†ä¸­: {word}")
                results.append({
                    'word': word.capitalize(),
                    'phonetic': get_phonetic(word),
                    'translation': translator.translate(word),
                    'context_en': word_pool[word],
                    'context_cn': translator.translate(word_pool[word])
                })
                time.sleep(0.3)
            except Exception as e:
                print(f"å–®å­— {word} è™•ç†å¤±æ•—: {e}")
        return results
    except Exception as e:
        print(f"æŠ“å–éç¨‹ç™¼ç”Ÿå…¨åŸŸéŒ¯èª¤: {e}")
        return []

def send_to_telegram(items):
    api_url = f"https://api.telegram.org/bot{BOT_TOKEN}/sendMessage"
    if not items:
        print("æ²’æœ‰å–®å­—å¯ç™¼é€ï¼Œæ­£åœ¨ç™¼é€æ¸¬è©¦è¨Šè™Ÿåˆ° Telegram...")
        requests.post(api_url, data={"chat_id": CHAT_ID, "text": "âš ï¸ æ©Ÿå™¨äººè­¦å‘Šï¼šä»Šæ—¥å–®å­—åº«ç¯©é¸å¾Œç‚ºç©ºï¼Œè«‹æª¢æŸ¥æ–°èæºã€‚"})
        return

    message = "<b>ä»Šæ—¥ BBC ç²¾é¸å–®å­—</b> ğŸ“š\n" + "-"*20 + "\n\n"
    for i, item in enumerate(items, 1):
        p = f" <code>{item['phonetic']}</code>" if item['phonetic'] else ""
        message += f"{i}. <b>{item['word']}</b>{p}\n   ğŸ”¹ {item['translation']}\n   ğŸ“ <i>{item['context_en']}</i>\n   ğŸ’¡ {item['context_cn']}\n\n"

    res = requests.post(api_url, data={"chat_id": CHAT_ID, "text": message, "parse_mode": "HTML"})
    print(f"Telegram ç™¼é€çµæœ: {res.status_code}")

if __name__ == "__main__":
    data = get_news_data()
    send_to_telegram(data)
