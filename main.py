import requests
from bs4 import BeautifulSoup
import re
from deep_translator import GoogleTranslator
import os
import nltk
import time
import random
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet
from nltk import ne_chunk, pos_tag, word_tokenize
import json
from datetime import datetime

# --- ç’°å¢ƒåˆå§‹åŒ– ---
for pkg in ['wordnet', 'averaged_perceptron_tagger', 'averaged_perceptron_tagger_eng', 
            'omw-1.4', 'punkt', 'punkt_tab', 'maxent_ne_chunker', 'maxent_ne_chunker_tab', 'words']:
    nltk.download(pkg, quiet=True)

BOT_TOKEN = os.environ.get('BOT_TOKEN')
CHAT_ID = os.environ.get('CHAT_ID')

def save_to_history(items):
    if not items:
        return
    
    file_path = 'history.json'
    # å–å¾—ä»Šå¤©æ—¥æœŸ (æ ¼å¼å¦‚ 2023-10-27)
    today = datetime.now().strftime('%Y-%m-%d')
    
    # æº–å‚™ä»Šå¤©è¦å„²å­˜çš„è³‡æ–™æ ¼å¼
    daily_record = []
    for item in items:
        daily_record.append({
            'word': item['word'],
            'phonetic': item['phonetic'],
            'translation': item['translation']
        })

    # è®€å–ç¾æœ‰çš„æ­·å²ç´€éŒ„
    history = {}
    if os.path.exists(file_path):
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                history = json.load(f)
        except:
            history = {}

    # æ›´æ–°æˆ–è¦†è“‹ç•¶å¤©çš„è³‡æ–™
    history[today] = daily_record

    # å¯«å›æª”æ¡ˆ (indent=2 è®“ JSON å¥½è®€ï¼Œensure_ascii=False ç¢ºä¿ä¸­æ–‡ä¸äº‚ç¢¼)
    with open(file_path, 'w', encoding='utf-8') as f:
        json.dump(history, f, ensure_ascii=False, indent=2)
    
    print(f"--- æ­·å²ç´€éŒ„æ›´æ–°å®Œæˆ ({today}) ---")

def get_manual_blacklist():
    blacklist = set()
    file_path = 'blacklist.txt'
    
    # å…§å»ºçµ•å°æ’é™¤
    internal_list = {'why', 'how', 'what', 'herself', 'himself'}
    blacklist.update(internal_list)
    
    if os.path.exists(file_path):
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                for line in f:
                    # å…ˆå»é™¤ç©ºç™½èˆ‡è¨»é‡‹
                    clean_line = line.strip().lower()
                    if not clean_line or clean_line.startswith('#'):
                        continue
                    
                    # é—œéµï¼šåŒæ™‚è™•ç†ã€Œé€—è™Ÿåˆ†éš”ã€èˆ‡ã€Œç©ºæ ¼åˆ†éš”ã€
                    # å…ˆæŠŠé€—è™Ÿæ›æˆç©ºæ ¼ï¼Œå†ç”¨ split() åˆ‡é–‹
                    words = clean_line.replace(',', ' ').split()
                    for w in words:
                        blacklist.add(w.strip())
            print(f"æˆåŠŸè¼‰å…¥ {len(blacklist)} å€‹é»‘åå–®å–®å­—ã€‚")
        except Exception as e:
            print(f"è®€å–å¤±æ•—: {e}")
    return blacklist

# åœ¨ä¸»é‚è¼¯ä¸­èª¿ç”¨
MANUAL_BLACKLIST = get_manual_blacklist()

def get_common_words(limit=5000):
    try:
        url = "https://raw.githubusercontent.com/first20hours/google-10000-english/master/google-10000-english-no-swears.txt"
        res = requests.get(url, timeout=10)
        return res.text.lower().splitlines()[:limit]
    except: return []

# é è¼‰å…¥å…©å€‹ç­‰ç´šçš„éæ¿¾å™¨
ALL_WORDS_SOURCE = get_common_words(5000)
FILTER_5000 = set(ALL_WORDS_SOURCE)
FILTER_3000 = set(ALL_WORDS_SOURCE[:3000])
lemmatizer = WordNetLemmatizer()

def lemmatize_word(word):
    try:
        tag = pos_tag([word])[0][1]
        tag_dict = {"J": wordnet.ADJ, "N": wordnet.NOUN, "V": wordnet.VERB, "R": wordnet.ADV}
        return lemmatizer.lemmatize(word, tag_dict.get(tag[0].upper(), wordnet.NOUN))
    except: return word

def filter_vocabulary(headlines, common_set):
    """é€šç”¨çš„å–®å­—ç¯©é¸é‚è¼¯"""
    word_pool = {}
    person_names = set()

    for sentence in headlines:
        # NER è¾¨è­˜äººåèˆ‡åœ°å
        tokens = word_tokenize(sentence)
        for chunk in ne_chunk(pos_tag(tokens)):
            if hasattr(chunk, 'label') and chunk.label() in ['PERSON', 'GPE', 'ORGANIZATION']:
                for leaf in chunk: person_names.add(leaf[0].lower())

        # æŠ“å– 4 å€‹å­—æ¯ä»¥ä¸Šçš„ç´”è‹±æ–‡å­—å–®å­—
        raw_words = re.findall(r'\b[a-zA-Z]{4,}\b', sentence)
        for rw in raw_words:
            word_clean = rw.lower().strip("'\"") # å¾¹åº•æ¸…é™¤å¼•è™Ÿ
            
            if word_clean in person_names or word_clean in common_set or word_clean in MANUAL_BLACKLIST:
                continue
            
            base = lemmatize_word(word_clean)
            if base not in common_set and base not in MANUAL_BLACKLIST and len(base) >= 4:
                if base not in word_pool:
                    word_pool[base] = sentence
    return word_pool

def get_news_data():
    url = "https://www.bbc.com/news"
    headers = {'User-Agent': 'Mozilla/5.0'}
    
    try:
        response = requests.get(url, headers=headers, timeout=15)
        soup = BeautifulSoup(response.text, 'html.parser')
        headlines = list(set([h.get_text().strip() for h in soup.find_all(['h2', 'h3']) if len(h.get_text().strip()) > 15]))
        
        # --- ç¬¬ä¸€å±¤ï¼š5000 å­—éæ¿¾ ---
        current_mode = "ç¬¬ä¸€å±¤ (5000å­—ç´šåˆ¥)"
        word_pool = filter_vocabulary(headlines, FILTER_5000)

        # --- ç¬¬äºŒå±¤ï¼šå¦‚æœä¸åˆ° 10 å€‹ï¼Œæ”¹ç”¨ 3000 å­—éæ¿¾ ---
        if len(word_pool) < 10:
            current_mode = "ç¬¬äºŒå±¤ (3000å­—ç´šåˆ¥ - é›£è©ä¸è¶³è‡ªå‹•é™ç´š)"
            word_pool = filter_vocabulary(headlines, FILTER_3000)

        candidate_keys = list(word_pool.keys())
        
        # --- Debug æ©Ÿåˆ¶ï¼šç§€å‡ºæ‰€æœ‰æŠ“åˆ°çš„å–®å­— ---
        print(f"--- ç³»çµ±è¨ºæ–·å ±å‘Š ---")
        print(f"ç•¶å‰æ¨¡å¼: {current_mode}")
        print(f"æ¨™é¡Œç¸½æ•¸: {len(headlines)}")
        print(f"å€™é¸å–®å­—ç¸½æ•¸: {len(candidate_keys)}")
        print(f"å®Œæ•´å€™é¸æ¸…å–®: {candidate_keys}")
        print(f"--------------------")

        if not candidate_keys: return []

        selected_keys = random.sample(candidate_keys, min(len(candidate_keys), 10))
        results = []
        translator = GoogleTranslator(source='en', target='zh-TW')
        
        for word in selected_keys:
            try:
                dict_url = f"https://api.dictionaryapi.dev/api/v2/entries/en/{word}"
                phonetic = ""
                d_res = requests.get(dict_url, timeout=5)
                if d_res.status_code == 200:
                    phonetic = d_res.json()[0].get('phonetic', "")

                results.append({
                    'word': word,
                    'phonetic': phonetic,
                    'translation': translator.translate(word),
                    'context_en': word_pool[word],
                    'context_cn': translator.translate(word_pool[word]),
                    'mode': current_mode # ç´€éŒ„ä¾†æºæ¨¡å¼
                })
                time.sleep(0.3)
            except: continue
        return results
    except Exception as e:
        print(f"Error: {e}"); return []

def send_to_telegram(items):
    if not items: return
    mode_info = items[0]['mode']
    message = f"<b>ä»Šæ—¥æ™‚äº‹å–®å­—åº« ({mode_info})</b> ğŸ“\n" + "-"*20 + "\n\n"
    for i, item in enumerate(items, 1):
        p = f" <code>{item['phonetic']}</code>" if item['phonetic'] else ""
        message += f"{i}. <b>{item['word']}</b>{p}\n   ğŸ”¹ {item['translation']}\n   ğŸ“ <i>{item['context_en']}</i>\n   ğŸ’¡ {item['context_cn']}\n\n"

    requests.post(f"https://api.telegram.org/bot{BOT_TOKEN}/sendMessage", 
                  data={"chat_id": CHAT_ID, "text": message, "parse_mode": "HTML"})

if __name__ == "__main__":
    data = get_news_data()
    if data:
        send_to_telegram(data)
        save_to_history(data)
